\subsection{Testing}
\label{Testing}
In order to test our software solution three different testing methods have been
arranged. Our test case is the ``University'' example as introduced in
Sec. \ref{background} and \ref{implementation}.


\subsubsection{Compilation}

To test code generation, our first step in validating our approach is checking
if the generated code compiles. If the code does not compile, we know that the
code generator does not generate valid code and that our templates are wrong.


\subsubsection{Manual Comparison}

A first approach to validating correctness is manual validation of generated log
files. A log file contains each action that occurs in a specified system with
local time, i.e. the clock time on the automaton, and global time, i.e. time
since the system started.

We employed this approach on the ``University'' example, together with Andrzej
Wasowski, who authored ECDAR and the given example specification.

\subsubsection{Log File Analysis}

As means to analyze the generated log files in a more automated and valid
fashion we conceived a JUnit test program. This program is tuned especially for
the ``University'' example and therefore does currently not apply to any other
specifications.

The analyzer converts a log file into a list of \textit{Events}, more exactly
\textit{Signals}, i.e. messages that are send from the environment or from automata
to the controller, which can be easily compared to in terms of type, source and
global time. These events are stored in a list sorted by time of occurrence. For
each occurring signal, there is a follow-up signal defined in the specification
(e.g. ``grant'' is followed by ``coin'') within a certain time frame.

The list of events is then searched until an occurrence of the follow-up signal
is found. For each signal encountered, one assertion for a follow-up signal is
issued. If a signal is missing its follow-up signal, the assertion evaluates to
false and the test case will result in an error.

We assume, that after one cycle through the specification, beginning with
``grant'' and ending with ``patent'', the system is behaving
correctly. Therefore, we let the test terminate as soon as ``patent'' is
encountered.

All runs of automated analysis for logs generated with our implementation
succeeded. However, it needs to be noted that currently, we do not verify the
timing constraints but only that the general behavior of the system is correct.

\subsection{Presumptions and Resulting Motivations}
\label{implementation-presumptions}

Our implementation represents only a subset of actual ECDAR. Currently, the
implementation assumes only one clock per automaton. Also, we assume the
specification to be valid, since there are other tools that verify
correctness\footnote{\url{http://people.cs.aau.dk/adavid/ecdar/}}.

The only operator we implement for code generation is the parallel
composition operator. Let $M$ be the type ECDAR specification. Then
all operators in ECDAR are of type $M_{i}\otimes M_{j}\rightarrow M_{ij}$.
Other than for the majority of operators, which refine the specification,
it is impractical to implement parallel composition as a model-to-model
transformation, since it produces the cross-product of two models
\cite{david_compositional_2012}. These models are size $|M_{i}|\cdot|M_{j}|$
and generating code for them would consume a large amount of memory
and raise complexity. This would be inappropriate for an embedded
system. We elaborate on this further in Sect. \ref{implementation-framework}.

ECDAR specifications are written on the assumption of the synchrony
hypothesis (see Sect. \ref{background-ecdar}) \cite{david_compositional_2012}.
This is an important property for code generation, as reasoning about
time differences in execution becomes unnecessary for the developer.
However, we still kept overhead low to achieve reasonable fast performance.



